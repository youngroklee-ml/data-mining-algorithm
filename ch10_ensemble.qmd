# Ensemble

## Example 10.5

### Load data

```{r}
dat <- read.csv("data/ch10_dat3.csv")
```

### Implement gradient boosting machine (GBM) for regression

Let us implement GBM regression estimation function by using `rpart::rpart()` trained on residuals in each iteration. When calling `rpart()`, pass `control = rpart::rpart.control(maxdepth = 1)` to let each tree will have up to only one split. This function will return a list of 1-depth decision trees, where the number of trees are determined by argument `ntree`. Assign additional class name `"my_gbm_reg"` to the return object, to implement a convenient prediction function later.

```{r}
gbm_regression <- function(formula, data, ntree = 5) {
  # get response variable name
  yvar <- all.vars(formula)[1]

  fit <- vector("list", length = ntree)
  for (i in seq_len(ntree)) {
    # fit a tree to predict negative gradients
    fit[[i]] <- rpart::rpart(
      formula,
      data = data,
      control = rpart::rpart.control(maxdepth = 1)
    )

    # prediction
    data$.pred <- predict(fit[[i]], newdata = data)
        
    # compute negative gradient as output
    data[[yvar]] <- data[[yvar]] - data$.pred
  }
  
  class(fit) <- c("my_gbm_reg", class(fit))
  fit
}
```

Let us train GBM on data.

```{r}
gbm_fit <- gbm_regression(Y ~ X, dat)
```


### Prediction

Let us implement a prediction function for our GBM regression by setting a function name to be `predict.my_gbm_reg()` according to S3 style. Here, `"my_gbm_reg"` is a class name for our model object.

```{r}
predict.my_gbm_reg <- function(object, newdata) {
  ntree <- length(object)
  rowSums(sapply(gbm_fit, predict, newdata = newdata))
}
```

Let's create prediction data that span range of X and plot prediction results as a line as well as training data as points.

```{r}
dat_p <- data.frame(
  X = seq(min(dat$X), max(dat$X), length = 1000)
)

plot(dat$X, dat$Y,
     pch = 16, xlab = "X", ylab = "Y",
     main = paste0("Observed vs Prediction")
)
lines(dat_p$X, predict(gbm_fit, newdata = dat_p), col = "red")
```


## Example 10.6

### Load data

```{r}
dat <- read.csv("data/ch8_dat1.csv")
dat$class <- dat$class - 1
```

### Implement gradient boosting machine (GBM) for binary classification

```{r}
gbm_classification <- function(formula, data, ntree = 10) {
  # get response variable name
  yvar <- all.vars(formula)[1]
  n <- nrow(data)
  
  # initialize
  score <- rep(0, n)
  posterior <- plogis(score) # exp(score) / (1 + exp(score))
  data[[yvar]] <- data[[yvar]] - posterior

  fit <- vector("list", length = ntree)
  for (i in seq_len(ntree)) {
    fit[[i]] <- rpart::rpart(
      formula,
      data = data,
      control = rpart::rpart.control(
        maxdepth = 1, minsplit = 1, cp = -Inf, minbucket = 1
      )
    )
    
    update <- 
      ave(data[[yvar]], fit[[i]]$where, FUN = sum) /
      ave(posterior, fit[[i]]$where, FUN = \(x) sum(x * (1 - x)))

    # convert back to original class value
    data[[yvar]] <- data[[yvar]] + posterior
        
    score <- score + update
    posterior <- plogis(score)
    data[[yvar]] <- data[[yvar]] - posterior
  }
  
  class(fit) <- "my_gbm_bin"
  fit
}
```


```{r}
gbm_fit <- gbm_classification(class ~ x1 + x2, dat)
```


### TO DO: Implement prediction function


