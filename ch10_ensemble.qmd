# Ensemble

## Example 10.5

### Load data

```{r}
dat <- read.csv("data/ch10_dat3.csv")
```

### Implement gradient boosting machine (GBM) for regression

Let us implement GBM regression estimation function by using `rpart::rpart()` trained on residuals in each iteration. When calling `rpart()`, pass `control = rpart::rpart.control(maxdepth = 1)` to let each tree will have up to only one split. This function will return a list of 1-depth decision trees, where the number of trees are determined by argument `ntree`. Assign additional class name `"my_gbm_reg"` to the return object, to implement a convenient prediction function later.

```{r}
gbm_regression <- function(formula, data, ntree = 5) {
  # get response variable name
  yvar <- all.vars(formula)[1]

  fit <- vector("list", length = ntree)
  for (i in seq_len(ntree)) {
    # fit a tree to predict negative gradients
    fit[[i]] <- rpart::rpart(
      formula,
      data = data,
      control = rpart::rpart.control(maxdepth = 1)
    )

    # prediction
    data$.pred <- predict(fit[[i]], newdata = data)
        
    # compute negative gradient as output
    data[[yvar]] <- data[[yvar]] - data$.pred
  }
  
  class(fit) <- c("my_gbm_reg", class(fit))
  fit
}
```

Let us train GBM on data.

```{r}
gbm_fit <- gbm_regression(Y ~ X, dat)
```


### Prediction

Let us implement a prediction function for our GBM regression by setting a function name to be `predict.my_gbm_reg()` according to S3 style. Here, `"my_gbm_reg"` is a class name for our model object.

```{r}
predict.my_gbm_reg <- function(object, newdata) {
  ntree <- length(object)
  rowSums(sapply(object, predict, newdata = newdata))
}
```

Let's create prediction data that span range of X and plot prediction results as a line as well as training data as points.

```{r}
dat_p <- data.frame(
  X = seq(min(dat$X), max(dat$X), length = 1000)
)

plot(dat$X, dat$Y,
     pch = 16, xlab = "X", ylab = "Y",
     main = paste0("Observed vs Prediction")
)
lines(dat_p$X, predict(gbm_fit, newdata = dat_p), col = "red")
```


## Example 10.6

### Load data

```{r}
dat <- read.csv("data/ch8_dat1.csv")
dat$class <- dat$class - 1
```

### Implement gradient boosting machine (GBM) for binary classification

```{r}
gbm_classification <- function(formula, data, ntree = 10) {
  # get response variable name
  yvar <- all.vars(formula)[1]
  
  # store observed response variable (binary class label)
  y_obs <- data[[yvar]]
  n <- length(y_obs)
  
  # initialize
  score <- rep(0, n)
  posterior <- plogis(score) # exp(score) / (1 + exp(score))
  data[[yvar]] <- y_obs - posterior

  fit <- vector("list", length = ntree)
  fit_update <- vector("list", length = ntree)

  for (i in seq_len(ntree)) {
    fit[[i]] <- rpart::rpart(
      formula,
      data = data,
      control = rpart::rpart.control(
        maxdepth = 1, minsplit = 1, cp = -Inf, minbucket = 1
      )
    )
    
    update <- 
      ave(data[[yvar]], fit[[i]]$where, FUN = sum) /
      ave(posterior, fit[[i]]$where, FUN = \(x) sum(x * (1 - x)))
    
    # create new tree for prediction
    fit_update[[i]] <- fit[[i]]
    fit_update[[i]]$frame$yval <- 
      c(mean(update),
        mean(update[fit[[i]]$where == 2]), 
        mean(update[fit[[i]]$where == 3]))

    score <- score + update
    posterior <- plogis(score)
    data[[yvar]] <- y_obs - posterior
  }
  
  class(fit_update) <- "my_gbm_bin"
  fit_update
}
```


```{r}
gbm_fit <- gbm_classification(class ~ x1 + x2, dat)
```


### Implement prediction function

```{r}
predict.my_gbm_bin <- function(object, newdata) {
  plogis(rowSums(sapply(object, predict, newdata = newdata)))
}
```


```{r}
dat_p <- expand.grid(
  x1 = seq(min(dat$x1), max(dat$x1), length = 100),
  x2 = seq(min(dat$x2), max(dat$x2), length = 100)
)

dat_p$posterior <- predict(gbm_fit, newdata = dat_p)
```

Draw heatmap of posterior probability by two variables.

```{r}
x1 <- unique(dat_p$x1)
x2 <- unique(dat_p$x2)
posterior <- matrix(dat_p$posterior, nrow = 100)
image(x = x1, y = x2, z = posterior)
```

