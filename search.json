[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining algorithm",
    "section": "",
    "text": "Preface\nThis is a Quarto book to provide step-by-step demonstration of data mining algorithm implementation to reproduce examples on https://youngroklee-ml.github.io/data-mining-techniques/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Ex 2.9: Stepwise",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#ex-2.9-stepwise",
    "href": "ch02_regression.html#ex-2.9-stepwise",
    "title": "Regression",
    "section": "",
    "text": "Load data\n\ndat_ba &lt;- read.csv(\"data/Hitters.csv\")\n\n\n\nCreate stepwise function\n\nstepwise &lt;- function(data, yvar, xvars, p_add = 0.3, p_drop = 0.3, niter = 100) {\n  # initialize\n  included = c()\n  remaining &lt;- setdiff(xvars, included)\n\n  # a function to create `lm()` formula\n  make_formula &lt;- function(yvar, xvars) {\n    if (length(xvars) == 0) {\n      res &lt;- as.formula(paste(yvar, \"~ 1\")) # intercept only\n    } else {\n      res &lt;- as.formula(paste(yvar, \"~\", paste(xvars, collapse = \"+\")))\n    }\n  \n    res\n  }\n\n  # initial model\n  current_model &lt;- lm(make_formula(yvar, included), data = data)\n\n  # iterative process  \n  for (i in seq_len(niter)) {\n    previously_included &lt;- included\n  \n    cat(\"Iteration\", i, \"forward step\\n\")\n  \n    # forward step\n    add_step &lt;- add1(\n      current_model, \n      scope = make_formula(yvar, xvars), \n      test = \"F\"\n    )\n  \n    # select a candidate with the largest F-statistics\n    add_candidate &lt;- which.max(add_step[[\"F value\"]])\n  \n    # update a model if addition criteria is satisfied\n    if (add_step[[\"Pr(&gt;F)\"]][add_candidate] &lt; p_add) {\n      varname &lt;- rownames(add_step)[add_candidate]\n      remaining &lt;- setdiff(remaining, varname)\n      included &lt;- union(included, varname)\n      current_model &lt;- lm(\n        make_formula(yvar, included),\n        data = data\n      )\n      cat(\". Add variable:\", varname, \"\\n\")\n    } else {\n      cat(\". No variable was added.\\n\")\n    }\n\n    cat(\"Iteration\", i, \"backward step\\n\")\n    # backward step\n    drop_step &lt;- drop1(\n      current_model, \n      scope = make_formula(yvar, included), \n      test = \"F\"\n    )\n  \n    # select a candidate with the smallest F-statistics\n    drop_candidate &lt;- which.min(drop_step[[\"F value\"]])\n  \n    # update a model if removal criteria is satisfied\n    if (drop_step[[\"Pr(&gt;F)\"]][drop_candidate] &gt; p_drop) {\n      varname &lt;- rownames(drop_step)[drop_candidate]\n      remaining &lt;- union(remaining, varname)\n      included &lt;- setdiff(included, varname)\n      current_model &lt;- lm(\n        make_formula(yvar, included),\n        data = data\n      )\n      cat(\". Add variable:\", varname, \"\\n\")\n    } else {\n      cat(\". No variable was removed.\\n\")\n    }\n    cat(\"----------------------------\\n\")\n\n    if (length(setdiff(included, previously_included)) == 0) { # no changes\n      break\n    }\n  \n    if (length(remaining) == 0) { # no additional candidate\n      break\n    }\n  }\n  \n  # return final model object\n  current_model\n}\n\n\n\nRun stepwise regression\nFour variables are added with default argument p_add = 0.3 and p_drop = 0.3.\n\nstepwise(\n  dat_ba, \n  yvar = \"Salary\", \n  xvars = c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\")\n)\n\nIteration 1 forward step\n. Add variable: CRuns \nIteration 1 backward step\n. No variable was removed.\n----------------------------\nIteration 2 forward step\n. Add variable: Hits \nIteration 2 backward step\n. No variable was removed.\n----------------------------\nIteration 3 forward step\n. Add variable: Walks \nIteration 3 backward step\n. No variable was removed.\n----------------------------\nIteration 4 forward step\n. Add variable: CWalks \nIteration 4 backward step\n. No variable was removed.\n----------------------------\nIteration 5 forward step\n. No variable was added.\nIteration 5 backward step\n. No variable was removed.\n----------------------------\n\n\n\nCall:\nlm(formula = make_formula(yvar, included), data = data)\n\nCoefficients:\n(Intercept)        CRuns         Hits        Walks       CWalks  \n   -63.6510       1.0223       1.5692       5.0583      -0.5644  \n\n\nFive variables are added with p_add = 0.5 and p_drop = 0.5.\n\nstepwise(\n  dat_ba, \n  yvar = \"Salary\", \n  xvars = c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\"),\n  p_add = 0.5,\n  p_drop = 0.5\n)\n\nIteration 1 forward step\n. Add variable: CRuns \nIteration 1 backward step\n. No variable was removed.\n----------------------------\nIteration 2 forward step\n. Add variable: Hits \nIteration 2 backward step\n. No variable was removed.\n----------------------------\nIteration 3 forward step\n. Add variable: Walks \nIteration 3 backward step\n. No variable was removed.\n----------------------------\nIteration 4 forward step\n. Add variable: CWalks \nIteration 4 backward step\n. No variable was removed.\n----------------------------\nIteration 5 forward step\n. Add variable: HmRun \nIteration 5 backward step\n. No variable was removed.\n----------------------------\n\n\n\nCall:\nlm(formula = make_formula(yvar, included), data = data)\n\nCoefficients:\n(Intercept)        CRuns         Hits        Walks       CWalks        HmRun  \n   -61.7447       1.0212       1.3558       4.9172      -0.5726       2.5378",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "Regularized regression",
    "section": "",
    "text": "Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "Regularized regression",
    "section": "",
    "text": "Load data\n\ndat1 &lt;- read.csv(\"data/ch3_dat1.csv\")\nx &lt;- as.matrix(dat1[, 1:2])\ny &lt;- dat1$y\n\n\n\nStandardize input data matrix\n\nstd_x &lt;- scale(x)\n\n\n\nEx 3.1: Lasso\nCreate lasso() function with three parameters:\n\nx: input data matrix\ny: a vector of response variable\nlambda: amount of regularization\n\n\nlasso &lt;- function(x, y, lambda) {\n  # add intercept coefficient\n  betas &lt;- rep(0, ncol(x) + 1)\n  \n  # optimize coefficient to minimize objective function\n  # passed as the second argument of `optim()`\n  # NOTE: do not regularize intercept\n  res &lt;- optim(\n    betas,\n    function(betas, x, y, lambda) {\n      sum((y - cbind(1, x) %*% betas)^2) + lambda * sum(abs(betas[-1]))\n    },\n    x = x,\n    y = y,\n    lambda = lambda,\n    method = \"BFGS\"\n  )\n  \n  # return cofficients estimates\n  res$par\n}\n\nLet’s apply to example data by varying lambda.\n\nlambda &lt;- seq(0, 3, by = 1)\n\nfor (i in lambda) {\n  print(round(lasso(std_x, y, i), 4))\n}\n\n[1] 0.0000 2.0201 0.1339\n[1] 0.0000 1.9736 0.0875\n[1] 0.0000 1.9272 0.0410\n[1] 0.0000 1.8757 0.0008\n\n\n\n\nEx 3.2: Ridge\nCreate ridge() function with the same three parameters:\n\nx: input data matrix\ny: a vector of response variable\nlambda: amount of regularization\n\nThe ridge() function is very similar to lasso(). The only difference is the regularization term in the objective function of optim() call.\n\nridge &lt;- function(x, y, lambda) {\n  # add intercept coefficient\n  betas &lt;- rep(0, ncol(x) + 1)\n  \n  # optimize coefficient to minimize objective function\n  # passed as the second argument of `optim()`\n  # NOTE: do not regularize intercept\n  res &lt;- optim(\n    betas,\n    function(betas, x, y, lambda) {\n      sum((y - cbind(1, x) %*% betas)^2) + lambda * sum((betas[-1])^2)\n    },\n    x = x,\n    y = y,\n    lambda = lambda,\n    method = \"BFGS\"\n  )\n  \n  # return cofficients estimates\n  res$par\n}\n\nLet’s apply to example data by varying lambda.\n\nlambda &lt;- seq(0, 3, by = 1)\n\nfor (i in lambda) {\n  print(round(ridge(std_x, y, i), 4))\n}\n\n[1] 0.0000 2.0201 0.1339\n[1] 0.0000 1.5071 0.4638\n[1] 0.0000 1.2688 0.5477\n[1] 0.0000 1.1177 0.5667",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "Dimension reduction",
    "section": "",
    "text": "Example 4.10",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.10",
    "href": "ch04_dimension_reduction.html#example-4.10",
    "title": "Dimension reduction",
    "section": "",
    "text": "Load data\n\ndat2 &lt;- read.csv(\"data/ch4_dat2.csv\", fileEncoding = \"euc-kr\")\nx &lt;- as.matrix(dat2[, 2:6])\n\n\n\nStandardize data\n\nscaled_x &lt;- scale(x)\n\n\n\nNIPALS algorithm\n\n#' @param X input data matrix\n#' @param ncomp number of principal components\nnipals_pca &lt;- function(X, ncomp = NULL) {\n  if (is.null(ncomp) || (ncomp &gt; min(dim(X)))) {\n    ncomp &lt;- min(dim(X))\n  }\n\n  Th &lt;- matrix(NA, nrow = nrow(X), ncol = ncomp)\n  Vh &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n\n  for (h in seq_len(ncomp)) {\n    # initialize h-th principal component score\n    j &lt;- sample(ncol(X), 1L)\n    Th[, h] &lt;- X[, j]\n    \n    while (TRUE) {\n      # compute h-th loading vector\n      Vh[, h] &lt;- t(t(Th[, h]) %*% X / (norm(Th[, h], \"2\")^2))\n      # normalize h-th loading vector\n      Vh[, h] &lt;- Vh[, h] / norm(Vh[, h], \"2\")\n      # compute new h-th principal component score\n      th &lt;- X %*% Vh[, h]\n      # check convergence\n      if (all(abs(Th[, h] - th) &lt; .Machine$double.eps^0.5)) break\n      # update h-th principal component score\n      Th[, h] &lt;- th\n    }\n    \n    # update input matrix by subtracting h-th principal component's contribution\n    X &lt;- X - Th[, h] %*% t(Vh[, h])\n  }\n\n  return(list(score = Th, loading = Vh))\n}\n\nLet’s call this function to estimate principal component scores and loadings:\n\nnipals_pca(scaled_x)\n\n$score\n            [,1]       [,2]        [,3]        [,4]         [,5]\n [1,]  1.4870243 -0.6066594  0.63361774  0.29625002 -0.020293733\n [2,]  0.2063797  0.0804627  0.04965017 -0.26323513 -0.063581471\n [3,] -0.1968538 -0.9704605  0.39507855 -0.27123747 -0.103351743\n [4,]  2.3542884  3.5056480 -0.16252733 -0.02524923  0.249920974\n [5,]  0.8953707 -1.4552899 -1.36265906 -0.20161775  0.055517169\n [6,]  0.3682082  0.5976313 -0.65857722 -0.27901317 -0.060458246\n [7,]  0.9354306  1.4144519  0.82574638 -0.07358977 -0.095960907\n [8,] -2.4129728  0.6785064 -0.92207606  0.36161577  0.062593518\n [9,] -2.6991862  0.7596591  0.45091078  0.21030377 -0.168645129\n[10,]  0.4050098 -0.2800099  0.92835441 -0.13993489 -0.001811117\n[11,] -1.3958199 -1.1353513  0.09819177 -0.34335126  0.094986799\n[12,]  1.5381192 -1.1576616  0.07467334  0.29404424 -0.052430948\n[13,] -0.3217681  0.2378023 -1.10180230 -0.28507243 -0.030666760\n[14,]  2.0306806 -0.9646122 -0.20906175  0.39639758  0.085778567\n[15,] -3.0389460 -0.8841645  0.77478769  0.04079855  0.349688462\n[16,] -2.0064063  1.2831337 -0.64388896  0.22077705 -0.188366873\n[17,]  0.4211779  0.2987099  1.20644766 -0.11766275 -0.068250990\n[18,]  1.4302634 -1.4017959 -0.37686579  0.17977686 -0.044667569\n\n$loading\n            [,1]        [,2]          [,3]         [,4]        [,5]\n[1,] -0.07608427  0.77966993 -0.0008915904  0.140755398 -0.60540326\n[2,]  0.39463007  0.56541218  0.2953216542 -0.117644164  0.65078503\n[3,] -0.56970191  0.16228156 -0.2412221042  0.637721895  0.42921684\n[4,]  0.55982770 -0.19654293  0.2565972879  0.748094309 -0.14992185\n[5,]  0.44778451  0.08636803 -0.8881182657  0.003668428  0.05711464\n\n\nLet’s derive eigenvalues from the results.\n\nnipals_pca(scaled_x)$score |&gt; \n  var() |&gt; \n  diag()\n\n[1] 2.76146225 1.60565318 0.55056305 0.06406316 0.01825836",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.14---4.15",
    "href": "ch04_dimension_reduction.html#example-4.14---4.15",
    "title": "Dimension reduction",
    "section": "Example 4.14 - 4.15",
    "text": "Example 4.14 - 4.15\n\nLoad data\n\ndat3 &lt;- read.csv(file = \"data/ch4_dat3.csv\")\nx &lt;- as.matrix(dat3[, 1:3])\ny &lt;- as.vector(dat3[, 4])\n\n\n\nData preprocessing\n\ncentered_x &lt;- scale(x, scale = FALSE)\ncentered_y &lt;- scale(y, scale = FALSE)\n\n\n\nNIPALS algorithm\n\n#' @param X input data matrix\n#' @param y response variable vector\n#' @param ncomp number of latent components\nnipals_plsr &lt;- function(X, y, ncomp = NULL) {\n  if (is.null(ncomp) || (ncomp &gt; min(dim(X)))) {\n    ncomp &lt;- min(dim(X))\n  }\n\n  Tmat &lt;- matrix(NA, nrow = nrow(X), ncol = ncomp)\n  colnames(Tmat) &lt;- paste0(\"LV\", seq_len(ncomp))\n\n  Wmat &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n  rownames(Wmat) &lt;- colnames(X)\n  colnames(Wmat) &lt;- colnames(Tmat)\n\n  Pmat &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n  rownames(Pmat) &lt;- colnames(X)\n  colnames(Pmat) &lt;- colnames(Tmat)\n\n  b &lt;- vector(\"numeric\", length = ncomp)\n  names(b) &lt;- colnames(Tmat)\n\n  for (a in seq_len(ncomp)) {\n    # compute weight vector for a-th component\n    Wmat[, a] &lt;- 1 / sum(y^2) * (t(X) %*% y)\n    # normalize weight vector\n    Wmat[, a] &lt;- Wmat[, a] / norm(Wmat[, a], \"2\")\n\n    # compute a-th latent variable vector of input\n    Tmat[, a] &lt;- X %*% Wmat[, a]\n\n    # compute a-th loading for input\n    Pmat[, a] &lt;- 1 / sum(Tmat[, a]^2) * (t(X) %*% Tmat[, a])\n\n    # normalize loading vector and adjust latent variable and weight vector\n    p_size &lt;- norm(Pmat[, a], \"2\")\n    Pmat[, a] &lt;- Pmat[, a] / p_size\n    Tmat[, a] &lt;- Tmat[, a] * p_size\n    Wmat[, a] &lt;- Wmat[, a] * p_size\n\n    # compute regression coefficient\n    b[a] &lt;- 1 / sum(Tmat[, a]^2) * sum(y * Tmat[, a])\n\n    # update input matrix and response vector by subtracting a-th latent portion\n    X &lt;- X - Tmat[, a] %*% t(Pmat[, a])\n    y &lt;- y - Tmat[, a] %*% t(b[a])\n  }\n\n  return(list(score = Tmat, weight = Wmat, loading_x = Pmat, loading_y = b))\n}\n\nLet’s call this function to estimate PLS model.\n\nnipals_plsr(centered_x, centered_y, ncomp = 2)\n\n$score\n            LV1        LV2\n[1,] -6.3243200 -2.0380766\n[2,] -7.8584372 -0.5965965\n[3,] -3.6317877  1.5429616\n[4,]  0.9079469  1.9745198\n[5,]  5.7294582  0.7158171\n[6,] 11.1771398 -1.5986253\n\n$weight\n          LV1       LV2\nx1  0.2817766 0.6579688\nx2  0.3130851 0.6388931\nx3 -0.9079469 0.4245052\n\n$loading_x\n          LV1       LV2\nx1  0.2537679 0.5424154\nx2  0.2858180 0.7279599\nx3 -0.9240724 0.4193565\n\n$loading_y\n     LV1      LV2 \n2.924570 2.464658",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html",
    "href": "ch06_logistic_regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Example 6.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#example-6.1",
    "href": "ch06_logistic_regression.html#example-6.1",
    "title": "Logistic regression",
    "section": "",
    "text": "Load data\n\ndat1 &lt;- read.csv(\"data/ch6_dat1.csv\")\ndat1$Class &lt;- factor(dat1$Class, levels = c(\"Average\", \"Excellent\"))\n\n\nx &lt;- model.matrix(~ Break + Sleep + Circle, data = dat1)\ny &lt;- as.numeric(dat1$Class) - 1\n\n\n\nDefine log-likelihood function\n\nfn_loglik &lt;- function(y, x, beta) {\n  beta &lt;- as.matrix(beta, ncol = 1)\n  lincomb &lt;- x %*% beta\n  loglik &lt;- sum(lincomb * y) - sum(log(1 + exp(lincomb)))\n  loglik\n}\n\n\n\nEstimate logistic regression model by using optim()\n\nlogistic_regression &lt;- function(y, x) {\n  # initial setting of regression coefficients at random\n  beta_init &lt;- runif(ncol(x))\n  \n  res &lt;- optim(\n    beta_init, \n    fn = fn_loglik,\n    y = y, \n    x = x, \n    method = \"BFGS\", \n    control = list(fnscale = -1) # maximization\n  )\n  \n  names(res$par) &lt;- colnames(x)\n  res$par\n}\n\n\nlogistic_regression(y, x)\n\n(Intercept)       Break       Sleep      Circle \n -30.538200    2.033200    3.473815    2.416327",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html",
    "href": "ch09_svm.html",
    "title": "Support vector machine",
    "section": "",
    "text": "Example 9.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.1",
    "href": "ch09_svm.html#example-9.1",
    "title": "Support vector machine",
    "section": "",
    "text": "Load data\n\ndat &lt;- read.csv(\"data/ch9_dat1.csv\")\ndat\n\n  x1 x2 class\n1  5  7     1\n2  4  3    -1\n3  7  8     1\n4  8  6     1\n5  3  6    -1\n6  2  5    -1\n7  6  6     1\n8  9  6     1\n9  5  4    -1\n\n\n\nx &lt;- as.matrix(dat[, c(\"x1\", \"x2\")])\ny &lt;- dat$class\n\n\n\nFormulate a dual problem of linear support vector machine\n\\[\n\\begin{split}\n\\min \\text{  } & -\\mathbf{d}^{\\top}\\boldsymbol{\\alpha} + \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\mathbf{D}\\boldsymbol{\\alpha}\\\\\n\\text{s.t. } & \\mathbf{y}^{\\top}\\boldsymbol{\\alpha} = 0\\\\\n             & \\boldsymbol{\\alpha} \\ge \\mathbf{0}\n\\end{split}\n\\] where\n\\[\n\\begin{split}\n\\mathbf{d} &= \\mathbf{1}_{N \\times 1}\\\\\n\\mathbf{D} &= \\mathbf{y}\\mathbf{y}^{\\top}\\mathbf{X}\\mathbf{X}^{\\top}\\\\\n\\mathbf{y} &= \\left[ \\begin{array}{c c c c} y_1 & y_2 & \\cdots & y_N \\end{array} \\right]^\\top\\\\\n\\mathbf{X} &= \\left[ \\begin{array}{c c c c} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_N \\end{array} \\right]^{\\top}\n\\end{split}\n\\]\n\n\nImplement quadratic optimization function for linear support vector machine\n\nlinear_svm_separable &lt;- function(y, x) {\n  N &lt;- length(y)\n  d &lt;- rep(1, N)\n  D &lt;- tcrossprod(y) * tcrossprod(x)\n  \n  # make symmetric positive definite matrix\n  D_pd &lt;- as.matrix(Matrix::nearPD(D, doSym = TRUE)$mat)\n  \n  A &lt;- cbind(y, diag(N))\n  bvec &lt;- rep(0, 1 + N)\n  \n  qp_res &lt;- quadprog::solve.QP(D_pd, d, A, bvec, meq = 1)\n  alpha &lt;- qp_res$solution\n  sv &lt;- which(alpha &gt; 1e-5)\n  obj_value &lt;- -qp_res$value\n  w &lt;- crossprod(x, y * alpha)\n  \n  ind &lt;- sv\n  b &lt;- mean((1 - y[ind] * (x[ind, ] %*% w)) / y[ind])\n\n  list(\n    obj_value = obj_value,\n    alpha = alpha,\n    sv = sv,\n    w = w,\n    b = b\n  )\n}\n\n\n\nEstimate a model\n\nlinear_svm_separable(y, x)\n\n$obj_value\n[1] 0.4444438\n\n$alpha\n[1]  2.234008e-01  7.809557e-12 -1.479354e-11  0.000000e+00  2.228114e-01\n[6] -1.504144e-11  2.210431e-01 -2.992210e-11  2.216325e-01\n\n$sv\n[1] 1 5 7 9\n\n$w\n        [,1]\nx1 0.6666658\nx2 0.6666657\n\n$b\n[1] -6.99999",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.2-inseparable",
    "href": "ch09_svm.html#example-9.2-inseparable",
    "title": "Support vector machine",
    "section": "Example 9.2: Inseparable",
    "text": "Example 9.2: Inseparable\n\nLoad data\n\ndat &lt;- read.csv(\"data/ch9_dat2.csv\")\ndat\n\n   x1 x2 class\n1   5  7     1\n2   4  3    -1\n3   7  8     1\n4   8  6     1\n5   3  6    -1\n6   2  5    -1\n7   6  6     1\n8   9  6     1\n9   5  4    -1\n10  7  6    -1\n\n\n\nx &lt;- as.matrix(dat[, c(\"x1\", \"x2\")])\ny &lt;- dat$class\n\n\n\nFormulate a dual problem of linear support vector machine\n\\[\n\\begin{split}\n\\min \\text{  } & -\\mathbf{d}^{\\top}\\boldsymbol{\\alpha} + \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\mathbf{D}\\boldsymbol{\\alpha}\\\\\n\\text{s.t. } & \\mathbf{y}^{\\top}\\boldsymbol{\\alpha} = 0\\\\\n             & \\boldsymbol{\\alpha} \\ge \\mathbf{0}\\\\\n             & \\boldsymbol{\\alpha} \\le C\\\\\n\\end{split}\n\\]\n\n\nImplement quadratic optimization function for linear support vector machine\n\nlinear_svm_inseparable &lt;- function(y, x, C) {\n  N &lt;- length(y)\n  d &lt;- rep(1, N)\n  D &lt;- tcrossprod(y) * tcrossprod(x)\n  \n  # make symmetric positive definite matrix\n  D_pd &lt;- as.matrix(Matrix::nearPD(D, doSym = TRUE)$mat)\n  \n  A &lt;- cbind(y, diag(N), -diag(N))\n  bvec &lt;- c(rep(0, 1 + N), rep(-C, N))\n  \n  qp_res &lt;- quadprog::solve.QP(D_pd, d, A, bvec, meq = 1)\n  alpha &lt;- qp_res$solution\n  sv &lt;- which(alpha &gt; 1e-5)\n  obj_value &lt;- -qp_res$value\n  w &lt;- crossprod(x, y * alpha)\n  \n  if (C &gt; 0) {\n    ind &lt;- sv[alpha[sv] &lt; C * (1 - sqrt(.Machine$double.eps))]\n  } else {\n    ind &lt;- sv\n  }\n  b &lt;- mean((1 - y[ind] * (x[ind, ] %*% w)) / y[ind])\n\n  list(\n    obj_value = obj_value,\n    alpha = alpha,\n    sv = sv,\n    w = w,\n    b = b\n  )\n}\n\n\n\nEstimate a model with C = 1\n\nlinear_svm_inseparable(y, x, C = 1)\n\n$obj_value\n[1] 3.100058\n\n$alpha\n [1]  7.999978e-01  3.165338e-14  2.178685e-10 -1.110220e-17  7.999978e-01\n [6] -3.129622e-17  1.000000e+00  1.050385e-16  0.000000e+00  1.000000e+00\n\n$sv\n[1]  1  5  7 10\n\n$w\n        [,1]\nx1 0.5999956\nx2 0.7999978\n\n$b\n[1] -7.599968",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html",
    "href": "ch10_ensemble.html",
    "title": "Ensemble",
    "section": "",
    "text": "Example 10.5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html#example-10.5",
    "href": "ch10_ensemble.html#example-10.5",
    "title": "Ensemble",
    "section": "",
    "text": "Load data\n\ndat &lt;- read.csv(\"data/ch10_dat3.csv\")\n\n\n\nImplement gradient boosting machine (GBM) for regression\nLet us implement GBM regression estimation function by using rpart::rpart() trained on residuals in each iteration. When calling rpart(), pass control = rpart::rpart.control(maxdepth = 1) to let each tree will have up to only one split. This function will return a list of 1-depth decision trees, where the number of trees are determined by argument ntree. Assign additional class name \"my_gbm_reg\" to the return object, to implement a convenient prediction function later.\n\ngbm_regression &lt;- function(formula, data, ntree = 5) {\n  # average prediction as initial model\n  yvar &lt;- all.vars(formula)[1]\n  offset &lt;- mean(data[[yvar]])\n  data$.pred &lt;- offset\n\n  fit &lt;- vector(\"list\", length = ntree)\n  for (i in seq_len(ntree)) {\n    # fit a tree to predict negative gradients\n    fit[[i]] &lt;- rpart::rpart(\n      formula,\n      data = data,\n      control = rpart::rpart.control(maxdepth = 1)\n    )\n\n    # prediction\n    data$.pred &lt;- predict(fit[[i]], newdata = data)\n        \n    # compute negative gradient as output\n    data[[yvar]] &lt;- data[[yvar]] - data$.pred\n  }\n  \n  class(fit) &lt;- c(\"my_gbm_reg\", class(fit))\n  fit\n}\n\nLet us train GBM on data.\n\ngbm_fit &lt;- gbm_regression(Y ~ X, dat)\n\nLet us implement a prediction function for our GBM regression by setting a function name to be predict.my_gbm_reg() according to S3 style. Here, \"my_gbm_reg\" is a class name for our model object.\n\npredict.my_gbm_reg &lt;- function(object, newdata) {\n  ntree &lt;- length(object)\n  rowSums(sapply(gbm_fit, predict, newdata = newdata))\n}\n\nLet’s create prediction data that span range of X and plot prediction results as a line as well as training data as points.\n\ndat_p &lt;- data.frame(\n  X = seq(min(dat$X), max(dat$X), length = 1000)\n)\n\nplot(dat$X, dat$Y,\n     pch = 16, xlab = \"X\", ylab = \"Y\",\n     main = paste0(\"Observed vs Prediction\")\n)\nlines(dat_p$X, predict(gbm_fit, newdata = dat_p), col = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  }
]