[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining algorithm",
    "section": "",
    "text": "Preface\nThis is a Quarto book to provide step-by-step demonstration of data mining algorithm implementation to reproduce examples on https://youngroklee-ml.github.io/data-mining-techniques/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Ex 2.9: Stepwise",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#ex-2.9-stepwise",
    "href": "ch02_regression.html#ex-2.9-stepwise",
    "title": "Regression",
    "section": "",
    "text": "Load data\n\ndat_ba &lt;- read.csv(\"data/Hitters.csv\")\n\n\n\nCreate stepwise function\n\nstepwise &lt;- function(data, yvar, xvars, p_add = 0.3, p_drop = 0.3, niter = 100) {\n  # initialize\n  included = c()\n  remaining &lt;- setdiff(xvars, included)\n\n  # a function to create `lm()` formula\n  make_formula &lt;- function(yvar, xvars) {\n    if (length(xvars) == 0) {\n      res &lt;- as.formula(paste(yvar, \"~ 1\")) # intercept only\n    } else {\n      res &lt;- as.formula(paste(yvar, \"~\", paste(xvars, collapse = \"+\")))\n    }\n  \n    res\n  }\n\n  # initial model\n  current_model &lt;- lm(make_formula(yvar, included), data = data)\n\n  # iterative process  \n  for (i in seq_len(niter)) {\n    previously_included &lt;- included\n  \n    cat(\"Iteration\", i, \"forward step\\n\")\n  \n    # forward step\n    add_step &lt;- add1(\n      current_model, \n      scope = make_formula(yvar, xvars), \n      test = \"F\"\n    )\n  \n    # select a candidate with the largest F-statistics\n    add_candidate &lt;- which.max(add_step[[\"F value\"]])\n  \n    # update a model if addition criteria is satisfied\n    if (add_step[[\"Pr(&gt;F)\"]][add_candidate] &lt; p_add) {\n      varname &lt;- rownames(add_step)[add_candidate]\n      remaining &lt;- setdiff(remaining, varname)\n      included &lt;- union(included, varname)\n      current_model &lt;- lm(\n        make_formula(yvar, included),\n        data = data\n      )\n      cat(\". Add variable:\", varname, \"\\n\")\n    } else {\n      cat(\". No variable was added.\\n\")\n    }\n\n    cat(\"Iteration\", i, \"backward step\\n\")\n    # backward step\n    drop_step &lt;- drop1(\n      current_model, \n      scope = make_formula(yvar, included), \n      test = \"F\"\n    )\n  \n    # select a candidate with the smallest F-statistics\n    drop_candidate &lt;- which.min(drop_step[[\"F value\"]])\n  \n    # update a model if removal criteria is satisfied\n    if (drop_step[[\"Pr(&gt;F)\"]][drop_candidate] &gt; p_drop) {\n      varname &lt;- rownames(drop_step)[drop_candidate]\n      remaining &lt;- union(remaining, varname)\n      included &lt;- setdiff(included, varname)\n      current_model &lt;- lm(\n        make_formula(yvar, included),\n        data = data\n      )\n      cat(\". Add variable:\", varname, \"\\n\")\n    } else {\n      cat(\". No variable was removed.\\n\")\n    }\n    cat(\"----------------------------\\n\")\n\n    if (length(setdiff(included, previously_included)) == 0) { # no changes\n      break\n    }\n  \n    if (length(remaining) == 0) { # no additional candidate\n      break\n    }\n  }\n  \n  # return final model object\n  current_model\n}\n\n\n\nRun stepwise regression\nFour variables are added with default argument p_add = 0.3 and p_drop = 0.3.\n\nstepwise(\n  dat_ba, \n  yvar = \"Salary\", \n  xvars = c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\")\n)\n\nIteration 1 forward step\n. Add variable: CRuns \nIteration 1 backward step\n. No variable was removed.\n----------------------------\nIteration 2 forward step\n. Add variable: Hits \nIteration 2 backward step\n. No variable was removed.\n----------------------------\nIteration 3 forward step\n. Add variable: Walks \nIteration 3 backward step\n. No variable was removed.\n----------------------------\nIteration 4 forward step\n. Add variable: CWalks \nIteration 4 backward step\n. No variable was removed.\n----------------------------\nIteration 5 forward step\n. No variable was added.\nIteration 5 backward step\n. No variable was removed.\n----------------------------\n\n\n\nCall:\nlm(formula = make_formula(yvar, included), data = data)\n\nCoefficients:\n(Intercept)        CRuns         Hits        Walks       CWalks  \n   -63.6510       1.0223       1.5692       5.0583      -0.5644  \n\n\nFive variables are added with p_add = 0.5 and p_drop = 0.5.\n\nstepwise(\n  dat_ba, \n  yvar = \"Salary\", \n  xvars = c(\"Hits\", \"Walks\", \"CRuns\", \"HmRun\", \"CWalks\"),\n  p_add = 0.5,\n  p_drop = 0.5\n)\n\nIteration 1 forward step\n. Add variable: CRuns \nIteration 1 backward step\n. No variable was removed.\n----------------------------\nIteration 2 forward step\n. Add variable: Hits \nIteration 2 backward step\n. No variable was removed.\n----------------------------\nIteration 3 forward step\n. Add variable: Walks \nIteration 3 backward step\n. No variable was removed.\n----------------------------\nIteration 4 forward step\n. Add variable: CWalks \nIteration 4 backward step\n. No variable was removed.\n----------------------------\nIteration 5 forward step\n. Add variable: HmRun \nIteration 5 backward step\n. No variable was removed.\n----------------------------\n\n\n\nCall:\nlm(formula = make_formula(yvar, included), data = data)\n\nCoefficients:\n(Intercept)        CRuns         Hits        Walks       CWalks        HmRun  \n   -61.7447       1.0212       1.3558       4.9172      -0.5726       2.5378",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "Regularized regression",
    "section": "",
    "text": "Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "Regularized regression",
    "section": "",
    "text": "Load data\n\ndat1 &lt;- read.csv(\"data/ch3_dat1.csv\")\nx &lt;- as.matrix(dat1[, 1:2])\ny &lt;- dat1$y\n\n\n\nStandardize input data matrix\n\nstd_x &lt;- scale(x)\n\n\n\nEx 3.1: Lasso\nCreate lasso() function with three parameters:\n\nx: input data matrix\ny: a vector of response variable\nlambda: amount of regularization\n\n\nlasso &lt;- function(x, y, lambda) {\n  # add intercept coefficient\n  betas &lt;- rep(0, ncol(x) + 1)\n  \n  # optimize coefficient to minimize objective function\n  # passed as the second argument of `optim()`\n  # NOTE: do not regularize intercept\n  res &lt;- optim(\n    betas,\n    function(betas, x, y, lambda) {\n      sum((y - cbind(1, x) %*% betas)^2) + lambda * sum(abs(betas[-1]))\n    },\n    x = x,\n    y = y,\n    lambda = lambda,\n    method = \"BFGS\"\n  )\n  \n  # return cofficients estimates\n  res$par\n}\n\nLet’s apply to example data by varying lambda.\n\nlambda &lt;- seq(0, 3, by = 1)\n\nfor (i in lambda) {\n  print(round(lasso(std_x, y, i), 4))\n}\n\n[1] 0.0000 2.0201 0.1339\n[1] 0.0000 1.9736 0.0875\n[1] 0.0000 1.9272 0.0410\n[1] 0.0000 1.8757 0.0008\n\n\n\n\nEx 3.2: Ridge\nCreate ridge() function with the same three parameters:\n\nx: input data matrix\ny: a vector of response variable\nlambda: amount of regularization\n\nThe ridge() function is very similar to lasso(). The only difference is the regularization term in the objective function of optim() call.\n\nridge &lt;- function(x, y, lambda) {\n  # add intercept coefficient\n  betas &lt;- rep(0, ncol(x) + 1)\n  \n  # optimize coefficient to minimize objective function\n  # passed as the second argument of `optim()`\n  # NOTE: do not regularize intercept\n  res &lt;- optim(\n    betas,\n    function(betas, x, y, lambda) {\n      sum((y - cbind(1, x) %*% betas)^2) + lambda * sum((betas[-1])^2)\n    },\n    x = x,\n    y = y,\n    lambda = lambda,\n    method = \"BFGS\"\n  )\n  \n  # return cofficients estimates\n  res$par\n}\n\nLet’s apply to example data by varying lambda.\n\nlambda &lt;- seq(0, 3, by = 1)\n\nfor (i in lambda) {\n  print(round(ridge(std_x, y, i), 4))\n}\n\n[1] 0.0000 2.0201 0.1339\n[1] 0.0000 1.5071 0.4638\n[1] 0.0000 1.2688 0.5477\n[1] 0.0000 1.1177 0.5667",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "Dimension reduction",
    "section": "",
    "text": "Example 4.10",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.10",
    "href": "ch04_dimension_reduction.html#example-4.10",
    "title": "Dimension reduction",
    "section": "",
    "text": "Load data\n\ndat2 &lt;- read.csv(\"data/ch4_dat2.csv\", fileEncoding = \"euc-kr\")\nx &lt;- as.matrix(dat2[, 2:6])\n\n\n\nStandardize data\n\nscaled_x &lt;- scale(x)\n\n\n\nNIPALS algorithm\n\n#' @param X input data matrix\n#' @param ncomp number of principal components\nnipals_pca &lt;- function(X, ncomp = NULL) {\n  if (is.null(ncomp) || (ncomp &gt; min(dim(X)))) {\n    ncomp &lt;- min(dim(X))\n  }\n\n  Th &lt;- matrix(NA, nrow = nrow(X), ncol = ncomp)\n  Vh &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n\n  for (h in seq_len(ncomp)) {\n    # initialize h-th principal component score\n    j &lt;- sample(ncol(X), 1L)\n    Th[, h] &lt;- X[, j]\n    \n    while (TRUE) {\n      # compute h-th loading vector\n      Vh[, h] &lt;- t(t(Th[, h]) %*% X / (norm(Th[, h], \"2\")^2))\n      # normalize h-th loading vector\n      Vh[, h] &lt;- Vh[, h] / norm(Vh[, h], \"2\")\n      # compute new h-th principal component score\n      th &lt;- X %*% Vh[, h]\n      # check convergence\n      if (all(abs(Th[, h] - th) &lt; .Machine$double.eps^0.5)) break\n      # update h-th principal component score\n      Th[, h] &lt;- th\n    }\n    \n    # update input matrix by subtracting h-th principal component's contribution\n    X &lt;- X - Th[, h] %*% t(Vh[, h])\n  }\n\n  return(list(score = Th, loading = Vh))\n}\n\nLet’s call this function to estimate principal component scores and loadings:\n\nnipals_pca(scaled_x)\n\n$score\n            [,1]       [,2]        [,3]        [,4]         [,5]\n [1,]  1.4870243 -0.6066594  0.63361774  0.29625002  0.020293735\n [2,]  0.2063797  0.0804627  0.04965018 -0.26323513  0.063581469\n [3,] -0.1968538 -0.9704605  0.39507856 -0.27123747  0.103351741\n [4,]  2.3542885  3.5056480 -0.16252734 -0.02524923 -0.249920974\n [5,]  0.8953707 -1.4552900 -1.36265905 -0.20161775 -0.055517170\n [6,]  0.3682082  0.5976313 -0.65857722 -0.27901317  0.060458244\n [7,]  0.9354306  1.4144519  0.82574637 -0.07358977  0.095960907\n [8,] -2.4129727  0.6785064 -0.92207607  0.36161577 -0.062593515\n [9,] -2.6991862  0.7596591  0.45091077  0.21030377  0.168645131\n[10,]  0.4050098 -0.2800099  0.92835441 -0.13993488  0.001811116\n[11,] -1.3958199 -1.1353513  0.09819178 -0.34335126 -0.094986801\n[12,]  1.5381192 -1.1576616  0.07467334  0.29404424  0.052430950\n[13,] -0.3217681  0.2378023 -1.10180230 -0.28507244  0.030666758\n[14,]  2.0306806 -0.9646122 -0.20906176  0.39639758 -0.085778564\n[15,] -3.0389460 -0.8841645  0.77478770  0.04079856 -0.349688462\n[16,] -2.0064062  1.2831337 -0.64388897  0.22077704  0.188366875\n[17,]  0.4211779  0.2987099  1.20644766 -0.11766274  0.068250990\n[18,]  1.4302634 -1.4017959 -0.37686579  0.17977686  0.044667571\n\n$loading\n            [,1]        [,2]          [,3]         [,4]        [,5]\n[1,] -0.07608427  0.77966993 -0.0008916028  0.140755394  0.60540326\n[2,]  0.39463007  0.56541218  0.2953216458 -0.117644152 -0.65078503\n[3,] -0.56970191  0.16228157 -0.2412221085  0.637721892 -0.42921683\n[4,]  0.55982770 -0.19654294  0.2565972888  0.748094315  0.14992187\n[5,]  0.44778451  0.08636802 -0.8881182671  0.003668407 -0.05711464\n\n\nLet’s derive eigenvalues from the results.\n\nnipals_pca(scaled_x)$score |&gt; \n  var() |&gt; \n  diag()\n\n[1] 2.76146225 1.60565318 0.55056305 0.06406316 0.01825836",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.14---4.15",
    "href": "ch04_dimension_reduction.html#example-4.14---4.15",
    "title": "Dimension reduction",
    "section": "Example 4.14 - 4.15",
    "text": "Example 4.14 - 4.15\n\nLoad data\n\ndat3 &lt;- read.csv(file = \"data/ch4_dat3.csv\")\nx &lt;- as.matrix(dat3[, 1:3])\ny &lt;- as.vector(dat3[, 4])\n\n\n\nData preprocessing\n\ncentered_x &lt;- scale(x, scale = FALSE)\ncentered_y &lt;- scale(y, scale = FALSE)\n\n\n\nNIPALS algorithm\n\n#' @param X input data matrix\n#' @param y response variable vector\n#' @param ncomp number of latent components\nnipals_plsr &lt;- function(X, y, ncomp = NULL) {\n  if (is.null(ncomp) || (ncomp &gt; min(dim(X)))) {\n    ncomp &lt;- min(dim(X))\n  }\n\n  Tmat &lt;- matrix(NA, nrow = nrow(X), ncol = ncomp)\n  colnames(Tmat) &lt;- paste0(\"LV\", seq_len(ncomp))\n\n  Wmat &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n  rownames(Wmat) &lt;- colnames(X)\n  colnames(Wmat) &lt;- colnames(Tmat)\n\n  Pmat &lt;- matrix(NA, nrow = ncol(X), ncol = ncomp)\n  rownames(Pmat) &lt;- colnames(X)\n  colnames(Pmat) &lt;- colnames(Tmat)\n\n  b &lt;- vector(\"numeric\", length = ncomp)\n  names(b) &lt;- colnames(Tmat)\n\n  for (a in seq_len(ncomp)) {\n    # compute weight vector for a-th component\n    Wmat[, a] &lt;- 1 / sum(y^2) * (t(X) %*% y)\n    # normalize weight vector\n    Wmat[, a] &lt;- Wmat[, a] / norm(Wmat[, a], \"2\")\n\n    # compute a-th latent variable vector of input\n    Tmat[, a] &lt;- X %*% Wmat[, a]\n\n    # compute a-th loading for input\n    Pmat[, a] &lt;- 1 / sum(Tmat[, a]^2) * (t(X) %*% Tmat[, a])\n\n    # normalize loading vector and adjust latent variable and weight vector\n    p_size &lt;- norm(Pmat[, a], \"2\")\n    Pmat[, a] &lt;- Pmat[, a] / p_size\n    Tmat[, a] &lt;- Tmat[, a] * p_size\n    Wmat[, a] &lt;- Wmat[, a] * p_size\n\n    # compute regression coefficient\n    b[a] &lt;- 1 / sum(Tmat[, a]^2) * sum(y * Tmat[, a])\n\n    # update input matrix and response vector by subtracting a-th latent portion\n    X &lt;- X - Tmat[, a] %*% t(Pmat[, a])\n    y &lt;- y - Tmat[, a] %*% t(b[a])\n  }\n\n  return(list(score = Tmat, weight = Wmat, loading_x = Pmat, loading_y = b))\n}\n\nLet’s call this function to estimate PLS model.\n\nnipals_plsr(centered_x, centered_y, ncomp = 2)\n\n$score\n            LV1        LV2\n[1,] -6.3243200 -2.0380766\n[2,] -7.8584372 -0.5965965\n[3,] -3.6317877  1.5429616\n[4,]  0.9079469  1.9745198\n[5,]  5.7294582  0.7158171\n[6,] 11.1771398 -1.5986253\n\n$weight\n          LV1       LV2\nx1  0.2817766 0.6579688\nx2  0.3130851 0.6388931\nx3 -0.9079469 0.4245052\n\n$loading_x\n          LV1       LV2\nx1  0.2537679 0.5424154\nx2  0.2858180 0.7279599\nx3 -0.9240724 0.4193565\n\n$loading_y\n     LV1      LV2 \n2.924570 2.464658",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html",
    "href": "ch06_logistic_regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "Example 6.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#example-6.1",
    "href": "ch06_logistic_regression.html#example-6.1",
    "title": "Logistic regression",
    "section": "",
    "text": "Load data\n\ndat1 &lt;- read.csv(\"data/ch6_dat1.csv\")\ndat1$Class &lt;- factor(dat1$Class, levels = c(\"Average\", \"Excellent\"))\n\n\nx &lt;- model.matrix(~ Break + Sleep + Circle, data = dat1)\ny &lt;- as.numeric(dat1$Class) - 1\n\n\n\nDefine log-likelihood function\n\nfn_loglik &lt;- function(y, x, beta) {\n  beta &lt;- as.matrix(beta, ncol = 1)\n  lincomb &lt;- x %*% beta\n  loglik &lt;- sum(lincomb * y) - sum(log(1 + exp(lincomb)))\n  loglik\n}\n\n\n\nEstimate logistic regression model by using optim()\n\nlogistic_regression &lt;- function(y, x) {\n  # initial setting of regression coefficients at random\n  beta_init &lt;- runif(ncol(x))\n  \n  res &lt;- optim(\n    beta_init, \n    fn = fn_loglik,\n    y = y, \n    x = x, \n    method = \"BFGS\", \n    control = list(fnscale = -1) # maximization\n  )\n  \n  names(res$par) &lt;- colnames(x)\n  res$par\n}\n\n\nlogistic_regression(y, x)\n\n(Intercept)       Break       Sleep      Circle \n -30.518592    2.031704    3.471592    2.414834",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  }
]